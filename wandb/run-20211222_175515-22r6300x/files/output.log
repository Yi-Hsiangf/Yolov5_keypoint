
Shape of passed values is (3051, 6), indices imply (3051, 4)
Overriding model.yaml nc=80 with nc=1
                 from  n    params  module                                  arguments
  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]
  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]
  2                -1  1     18816  models.common.C3                        [64, 64, 1]
  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]
  4                -1  2    115712  models.common.C3                        [128, 128, 2]
  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]
  6                -1  3    625152  models.common.C3                        [256, 256, 3]
  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]
  8                -1  1   1182720  models.common.C3                        [512, 512, 1]
  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]
 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]
 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 12           [-1, 6]  1         0  models.common.Concat                    [1]
 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]
 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]
 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 16           [-1, 4]  1         0  models.common.Concat                    [1]
 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]
 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]
 19          [-1, 14]  1         0  models.common.Concat                    [1]
 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]
 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]
 22          [-1, 10]  1         0  models.common.Concat                    [1]
 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]
 24      [17, 20, 23]  1     21576  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]
Model Summary: 270 layers, 7027720 parameters, 7027720 gradients, 15.9 GFLOPs
Transferred 343/349 items from yolov5s.pt
Scaled weight_decay = 0.0005
[34m[1moptimizer:[39m[22m SGD with parameter groups 57 weight, 60 weight (no decay), 60 bias
[34m[1mtrain: [39m[22mScanning '/home/data/data/2020_Autumn_key/labels/train.cache' images and labels... 1078 found, 0 missing, 239 empty, 2 corrupted: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1078/1078 [00:00<?, ?it/s]
[34m[1mtrain: [39m[22mWARNING: /home/data/data/2020_Autumn_key/images/train/VID_20201101_102143.mp4#t=144.2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1329      1.1329]
[34m[1mtrain: [39m[22mWARNING: /home/data/data/2020_Autumn_key/images/train/VID_20201101_102625.mp4#t=139.5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0273      1.0273]
[34m[1mval: [39m[22mScanning '/home/data/data/2020_Autumn_key/labels/val.cache' images and labels... 236 found, 0 missing, 66 empty, 0 corrupted: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 236/236 [00:00<?, ?it/s]
Plotting labels to runs/train/exp27/labels.jpg...
[34m[1mAutoAnchor: [39m[22m5.72 anchors/target, 0.986 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to [1mruns/train/exp27
Starting training for 150 epochs...
     Epoch   gpu_mem       box       obj       cls    labels  img_size
  0%|          | 0/34 [00:00<?, ?it/s]
p_keyxy.shape:  torch.Size([1321, 8])
pxy:  tensor([[ 0.48926, -0.13318],
        [-1.79004,  0.75146]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[14.46094, 13.56250],
        [13.47656, 14.45312]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([3240, 8])
pxy:  tensor([[ 1.18164, -0.64307],
        [ 0.72266,  0.65088]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.66406, 11.49219],
        [12.94531, 10.83594]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2226, 8])
pxy:  tensor([[ 0.37036, -0.07288],
        [ 0.32104,  0.44263]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.19531, 13.39062],
        [13.35938, 13.09375]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([2.36159], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03723], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([37.18041], device='cuda:0', grad_fn=<AddBackward0>)

     0/149     8.46G    0.1181   0.03723    0.9295       439      1280:   3%|‚ñé         | 1/34 [00:10<05:50, 10.62s/it]
p_keyxy.shape:  torch.Size([890, 8])
pxy:  tensor([[-1.70703,  0.03586],
        [ 0.05048, -0.01001]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.65625, 14.57812],
        [13.17969, 13.65625]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2371, 8])
pxy:  tensor([[-0.74219,  0.39600],
        [ 1.01074,  0.53516]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.97656, 11.32031],
        [13.66406, 12.57031]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1709, 8])
pxy:  tensor([[1.08105, 0.12988],
        [0.57275, 0.38232]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.32031, 14.46875],
        [13.54688, 13.92969]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([2.33430], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03369], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([37.14163], device='cuda:0', grad_fn=<AddBackward0>)

     0/149     7.68G    0.1175   0.03462     0.925       329      1280:  12%|‚ñà‚ñè        | 4/34 [00:12<01:00,  2.02s/it]
p_keyxy.shape:  torch.Size([1200, 8])
pxy:  tensor([[ 0.84814,  0.64404],
        [-0.28101, -0.03619]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.56250, 13.22656],
        [13.58594, 15.57031]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2805, 8])
pxy:  tensor([[ 0.12537, -0.14001],
        [ 0.70264,  0.25977]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.45312, 13.12500],
        [14.50781, 12.57812]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1857, 8])
pxy:  tensor([[ 0.13171, -0.04669],
        [ 0.72363, -0.93652]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.38281, 13.14062],
        [14.00781, 13.87500]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([2.36877], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03449], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([36.88161], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.92204], device='cuda:0', grad_fn=<MulBackward0>)
p_keyxy.shape:  torch.Size([1130, 8])
pxy:  tensor([[-0.41162, -1.03613],
        [ 0.11414,  0.06702]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.14062, 13.70312],
        [13.42969, 13.00781]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2480, 8])
pxy:  tensor([[-0.64258, -0.53027],
        [-0.00485, -0.44580]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.05469, 12.64844],
        [11.21875, 12.76562]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1603, 8])
pxy:  tensor([[0.52832, 0.29370],
        [0.24084, 0.23376]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.74219, 13.08594],
        [13.54688, 13.35156]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([2.33190], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03308], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([36.79817], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.91995], device='cuda:0', grad_fn=<MulBackward0>)
p_keyxy.shape:  torch.Size([1210, 8])
pxy:  tensor([[ 0.25903, -0.23450],
        [-1.06836,  0.63770]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[14.58594, 13.87500],
        [14.97656, 13.08594]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2934, 8])
pxy:  tensor([[0.68750, 0.69824],
        [1.33008, 0.72900]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.48438, 11.87500],
        [11.94531, 12.61719]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1996, 8])
pxy:  tensor([[ 0.16321, -0.42017],
        [ 0.10645,  1.33398]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[14.10938, 12.94531],
        [13.07031, 13.45312]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([2.31222], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03471], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([36.61637], device='cuda:0', grad_fn=<AddBackward0>)

     0/149     7.22G    0.1162   0.03463    0.9175       403      1280:  21%|‚ñà‚ñà        | 7/34 [00:14<00:27,  1.03s/it]
p_keyxy.shape:  torch.Size([1342, 8])
pxy:  tensor([[-1.03125, -0.77295],
        [-0.37842, -0.74805]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.32031, 13.43750],
        [13.07812, 14.00000]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2892, 8])
pxy:  tensor([[ 0.60449, -0.39380],
        [ 0.23181, -0.22974]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.31250, 11.78906],
        [13.90625, 11.87500]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1836, 8])
pxy:  tensor([[ 0.36743,  0.46924],
        [-0.46313,  0.50391]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.07812, 14.36719],
        [12.79688, 13.36719]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([2.28151], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03452], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([36.26935], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.90673], device='cuda:0', grad_fn=<MulBackward0>)
p_keyxy.shape:  torch.Size([1382, 8])
pxy:  tensor([[-0.48779,  0.35352],
        [ 1.00586,  0.27881]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.04688, 13.25781],
        [12.26562, 13.84375]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([3028, 8])
pxy:  tensor([[ 1.10352,  0.31934],
        [ 0.69189, -0.21240]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[14.39062, 11.78906],
        [12.36719, 11.88281]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1946, 8])
pxy:  tensor([[ 0.80469,  0.42871],
        [-0.13635, -1.08301]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.54688, 13.38281],
        [13.82812, 13.16406]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([2.27538], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03468], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([36.00661], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.90017], device='cuda:0', grad_fn=<MulBackward0>)
p_keyxy.shape:  torch.Size([1065, 8])
pxy:  tensor([[-0.58496, -0.40649],
        [ 0.08789, -0.73584]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[11.46875, 14.09375],
        [13.81250, 14.12500]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2412, 8])
pxy:  tensor([[-0.06088, -0.30078],
        [-1.11328, -1.04004]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[11.67188, 13.25781],
        [11.64062, 13.14062]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1576, 8])
pxy:  tensor([[ 0.45996,  0.43140],
        [ 0.15283, -0.49194]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.14844, 12.90625],
        [12.81250, 12.08594]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([2.26354], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03125], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([35.66048], device='cuda:0', grad_fn=<AddBackward0>)

     0/149     21.7G    0.1146   0.03409    0.9077       407      1280:  29%|‚ñà‚ñà‚ñâ       | 10/34 [00:16<00:17,  1.40it/s]
p_keyxy.shape:  torch.Size([1128, 8])
pxy:  tensor([[ 1.23145, -0.23828],
        [ 0.20569, -0.89648]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.91406, 13.57031],
        [13.87500, 14.02344]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2694, 8])
pxy:  tensor([[-0.08966, -0.77979],
        [-1.09668, -0.47900]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.09375, 12.45312],
        [12.10156, 11.87500]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1810, 8])
pxy:  tensor([[ 0.19934, -0.39160],
        [ 0.17432, -0.19604]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.85156, 13.62500],
        [13.52344, 12.65625]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([2.21863], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03254], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([35.39843], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.88496], device='cuda:0', grad_fn=<MulBackward0>)
p_keyxy.shape:  torch.Size([1233, 8])
pxy:  tensor([[ 0.24365, -0.88086],
        [-0.05414,  0.11182]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.76562, 13.48438],
        [13.45312, 13.26562]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([3051, 8])
pxy:  tensor([[-0.59473,  0.26367],
        [-0.85645,  0.57227]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[11.60938, 11.96875],
        [13.17188, 12.32031]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2038, 8])
pxy:  tensor([[ 0.05887,  0.14966],
        [-0.41577,  0.24219]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.56250, 12.06250],
        [12.57812, 12.17969]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([2.17089], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03473], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([35.12867], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.87822], device='cuda:0', grad_fn=<MulBackward0>)
p_keyxy.shape:  torch.Size([1195, 8])
pxy:  tensor([[-0.12524, -0.71338],
        [ 0.72559, -0.38916]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.76562, 13.14844],
        [14.18750, 14.94531]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2833, 8])
pxy:  tensor([[ 0.39697, -0.20874],
        [ 0.45239,  0.56934]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.76562, 10.47656],
        [12.13281, 12.17969]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1882, 8])
pxy:  tensor([[0.75977, 0.47900],
        [0.42969, 0.35229]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[11.89844, 12.42969],
        [11.90625, 12.16406]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([2.16624], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03313], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([34.72738], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.86818], device='cuda:0', grad_fn=<MulBackward0>)
p_keyxy.shape:  torch.Size([1237, 8])
pxy:  tensor([[ 0.31055,  1.27930],
        [-0.54639,  0.98584]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.32031, 13.32031],
        [12.70312, 13.78906]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2712, 8])
pxy:  tensor([[-0.71973,  0.69580],
        [-0.42529, -0.66943]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.44531, 11.30469],
        [12.59375, 12.19531]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1732, 8])
pxy:  tensor([[-0.04037, -0.31836],
        [ 0.58496,  0.56445]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.82031, 11.96094],
        [12.78125, 12.97656]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([2.13763], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03264], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([34.36884], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.85922], device='cuda:0', grad_fn=<MulBackward0>)
p_keyxy.shape:  torch.Size([1271, 8])
pxy:  tensor([[ 0.33130, -0.45996],
        [ 0.37158,  0.50928]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.47656, 12.58594],
        [13.89844, 14.65625]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2946, 8])
pxy:  tensor([[-0.38599,  0.26611],
        [-0.25586, -1.14844]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.66406, 12.25781],
        [11.95312, 12.16406]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2047, 8])
pxy:  tensor([[ 0.36719,  0.23474],
        [ 0.11377, -0.21313]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[11.97656, 12.30469],
        [12.63281, 12.55469]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([2.12527], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03411], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([34.00352], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.85009], device='cuda:0', grad_fn=<MulBackward0>)
p_keyxy.shape:  torch.Size([1185, 8])
pxy:  tensor([[-0.66260,  0.33789],
        [-0.02341,  0.06189]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.55469, 13.90625],
        [13.51562, 14.28906]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2914, 8])
pxy:  tensor([[ 1.41016, -0.01804],
        [ 0.03561,  0.22607]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.44531, 10.99219],
        [12.29688, 11.39062]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1980, 8])
pxy:  tensor([[-0.50537,  0.06421],
        [ 0.67676,  0.64746]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.34375, 12.10938],
        [12.03906, 12.82031]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([2.09764], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03350], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([33.54727], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.83868], device='cuda:0', grad_fn=<MulBackward0>)
p_keyxy.shape:  torch.Size([1093, 8])
pxy:  tensor([[ 1.34766,  0.18994],
        [-1.59766,  0.23059]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[14.73438, 14.29688],
        [13.59375, 12.92969]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2702, 8])
pxy:  tensor([[-0.88721, -0.56689],
        [ 1.10156, -0.69580]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[11.17969, 10.91406],
        [12.57031, 11.82812]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1897, 8])
pxy:  tensor([[ 0.64648, -0.24780],
        [ 0.53174, -1.10645]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.27344, 11.78125],
        [12.32031, 11.57812]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([2.03792], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03301], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([33.16866], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.82922], device='cuda:0', grad_fn=<MulBackward0>)
p_keyxy.shape:  torch.Size([1247, 8])
pxy:  tensor([[-0.54541,  0.22229],
        [ 0.07434, -0.72119]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[14.31250, 14.00000],
        [13.57031, 14.50781]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2862, 8])
pxy:  tensor([[-0.17273, -0.20374],
        [-1.15430,  0.67480]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.25000, 11.72656],
        [13.07812, 11.28125]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1929, 8])
pxy:  tensor([[0.71924, 0.56104],
        [0.38306, 0.58887]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.05469, 11.67969],
        [12.04688, 11.48438]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([2.02153], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03389], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([33.08488], device='cuda:0', grad_fn=<AddBackward0>)


     0/149     21.7G    0.1092   0.03363    0.8717       376      1280:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 19/34 [00:20<00:07,  2.04it/s]
p_keyxy.shape:  torch.Size([1196, 8])
pxy:  tensor([[-0.23682,  0.44800],
        [ 0.03284,  0.63281]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[14.54688, 13.67969],
        [12.32812, 12.60156]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2800, 8])
pxy:  tensor([[ 0.87598,  0.28735],
        [-0.55273, -0.28125]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.04688, 11.00781],
        [11.25781, 11.13281]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1889, 8])
pxy:  tensor([[-0.09302,  0.41016],
        [ 0.58984,  0.05978]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[11.64844, 12.91406],
        [11.42969, 12.60156]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([2.00604], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03315], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([32.54198], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.81355], device='cuda:0', grad_fn=<MulBackward0>)
p_keyxy.shape:  torch.Size([1044, 8])
pxy:  tensor([[ 0.62305, -0.32153],
        [-1.44141,  1.47949]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.59375, 14.71875],
        [13.08594, 14.10156]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2478, 8])
pxy:  tensor([[-0.56396, -1.07715],
        [-0.42456, -0.45386]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[10.57031, 11.20312],
        [12.07031, 11.03125]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1664, 8])
pxy:  tensor([[ 0.39893, -0.26245],
        [-0.00385,  0.21313]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[11.53125, 12.07031],
        [11.41406, 12.80469]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([1.97715], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03119], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([32.03687], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.80092], device='cuda:0', grad_fn=<MulBackward0>)
p_keyxy.shape:  torch.Size([1283, 8])
pxy:  tensor([[ 1.44434, -0.64844],
        [ 0.83008, -0.52197]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.97656, 12.40625],
        [13.85938, 13.89844]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2878, 8])
pxy:  tensor([[ 0.33691, -0.34961],
        [ 0.18469, -0.23975]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.10156, 11.39844],
        [11.34375, 11.54688]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1853, 8])
pxy:  tensor([[ 0.22241,  0.22607],
        [ 0.13208, -0.04309]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[11.14062, 11.37500],
        [11.86719, 11.65625]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([1.99343], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03336], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([31.90836], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.79771], device='cuda:0', grad_fn=<MulBackward0>)
p_keyxy.shape:  torch.Size([1287, 8])
pxy:  tensor([[ 0.49463, -0.05103],
        [-1.05371,  0.58984]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.14844, 13.31250],
        [11.97656, 13.70312]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2949, 8])
pxy:  tensor([[-1.65430,  0.14880],
        [-1.03906, -0.04718]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.30469, 11.28125],
        [12.10938, 11.35938]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1930, 8])
pxy:  tensor([[-0.03384,  0.15576],
        [-0.34521,  0.17920]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[11.25000, 10.92188],
        [11.23438, 11.17969]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([1.95901], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03423], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([31.20232], device='cuda:0', grad_fn=<AddBackward0>)

     0/149     21.7G    0.1072   0.03359    0.8527       386      1280:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 23/34 [00:22<00:05,  2.05it/s]
p_keyxy.shape:  torch.Size([1284, 8])
pxy:  tensor([[ 0.03412, -0.41479],
        [-0.73193, -0.39429]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.71094, 14.44531],
        [13.08594, 12.74219]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2914, 8])
pxy:  tensor([[-0.72070,  0.29395],
        [-0.41992, -0.02374]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.11719, 11.94531],
        [12.29688, 11.80469]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1811, 8])
pxy:  tensor([[ 0.65088,  0.34521],
        [ 0.08063, -0.43164]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[10.86719, 10.88281],
        [11.14062, 10.77344]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([1.97118], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03335], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([30.69190], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.76730], device='cuda:0', grad_fn=<MulBackward0>)
p_keyxy.shape:  torch.Size([1184, 8])
pxy:  tensor([[-0.73438, -0.65527],
        [-0.37524, -0.69580]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.54688, 13.56250],
        [12.19531, 12.76562]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2691, 8])
pxy:  tensor([[-0.61914,  1.00293],
        [ 0.31616, -0.11249]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.42188, 11.00000],
        [ 9.90625, 10.16406]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1766, 8])
pxy:  tensor([[ 0.04358, -0.27783],
        [ 0.04248, -0.06958]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[11.02344, 11.01562],
        [10.85156, 11.04688]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([1.94615], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03234], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([30.37334], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.75933], device='cuda:0', grad_fn=<MulBackward0>)
p_keyxy.shape:  torch.Size([1280, 8])
pxy:  tensor([[-1.53027,  1.29590],
        [-0.77393,  0.37549]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[13.78906, 13.64062],
        [12.64062, 14.44531]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2899, 8])
pxy:  tensor([[ 0.19238, -0.38965],
        [-1.34863, -0.87744]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[10.96094, 10.46094],
        [10.42969, 10.94531]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1973, 8])
pxy:  tensor([[ 0.59131,  0.69238],
        [ 0.15356, -0.76514]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[11.39062,  9.99219],
        [11.32812, 10.68750]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([1.95693], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03366], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([29.75984], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.74400], device='cuda:0', grad_fn=<MulBackward0>)
p_keyxy.shape:  torch.Size([1279, 8])
pxy:  tensor([[-1.68164, -0.16394],
        [-0.67773, -0.74805]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[11.80469, 12.09375],
        [15.17188, 12.64062]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2936, 8])
pxy:  tensor([[-0.46875, -1.09277],
        [-0.38989, -0.75977]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[11.55469, 11.42188],
        [11.56250,  9.70312]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2039, 8])
pxy:  tensor([[ 0.48877, -0.16736],
        [ 0.52539,  0.02695]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[10.96875, 10.82812],
        [ 9.95312, 10.71875]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([1.93385], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03416], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([29.41069], device='cuda:0', grad_fn=<AddBackward0>)

     0/149     21.7G    0.1068   0.03361    0.8478       384      1280:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 24/34 [00:22<00:04,  2.07it/s]
p_keyxy.shape:  torch.Size([1132, 8])
pxy:  tensor([[-0.38306, -0.14099],
        [ 1.28906,  0.58936]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[14.21875, 12.91406],
        [13.14062, 13.25000]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2695, 8])
pxy:  tensor([[-0.28271,  0.44995],
        [-1.71973, -0.01828]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[10.64062, 11.09375],
        [12.83594, 10.03125]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1848, 8])
pxy:  tensor([[-0.82031, -0.25488],
        [ 0.49512, -0.40771]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[10.68750, 11.00000],
        [10.60938, 10.35156]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([1.92768], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03196], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([28.73968], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.71849], device='cuda:0', grad_fn=<MulBackward0>)
p_keyxy.shape:  torch.Size([1178, 8])
pxy:  tensor([[ 1.00781, -0.60059],
        [-0.96387,  0.58447]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.82031, 12.05469],
        [14.13281, 13.14844]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2808, 8])
pxy:  tensor([[-1.26660, -0.07874],
        [ 0.10577, -0.56152]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[11.47656, 10.81250],
        [10.46094, 10.39062]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1828, 8])
pxy:  tensor([[0.30054, 0.35474],
        [0.75781, 0.91553]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[ 9.21875, 10.21875],
        [ 9.44531, 10.95312]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([1.91660], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03265], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([28.24910], device='cuda:0', grad_fn=<AddBackward0>)
     0/149     21.7G    0.1059   0.03351    0.8374       377      1280:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 26/34 [00:25<00:07,  1.01it/s]
Traceback (most recent call last):
  File "/home/rumex/keypoint_yolo/train.py", line 626, in <module>
    main(opt)
  File "/home/rumex/keypoint_yolo/train.py", line 523, in main
    train(opt.hyp, opt, device, callbacks)
  File "/home/rumex/keypoint_yolo/train.py", line 331, in train
    scaler.step(optimizer)  # optimizer.step
  File "/home/rumex/anaconda3/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 338, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/home/rumex/anaconda3/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 284, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "/home/rumex/anaconda3/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 284, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
KeyboardInterrupt
Exception in thread Thread-13:
Traceback (most recent call last):
  File "/home/rumex/anaconda3/lib/python3.9/threading.py", line 973, in _bootstrap_inner
    self.run()
  File "/home/rumex/anaconda3/lib/python3.9/threading.py", line 910, in run
    self._target(*self._args, **self._kwargs)
  File "/home/rumex/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py", line 28, in _pin_memory_loop
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/home/rumex/anaconda3/lib/python3.9/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/home/rumex/anaconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py", line 289, in rebuild_storage_fd
    fd = df.detach()
  File "/home/rumex/anaconda3/lib/python3.9/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/home/rumex/anaconda3/lib/python3.9/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/home/rumex/anaconda3/lib/python3.9/multiprocessing/connection.py", line 513, in Client
    answer_challenge(c, authkey)
  File "/home/rumex/anaconda3/lib/python3.9/multiprocessing/connection.py", line 757, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/home/rumex/anaconda3/lib/python3.9/multiprocessing/connection.py", line 221, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/rumex/anaconda3/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
  File "/home/rumex/anaconda3/lib/python3.9/multiprocessing/connection.py", line 384, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
p_keyxy.shape:  torch.Size([1266, 8])
pxy:  tensor([[ 0.20361,  0.96484],
        [-1.04492,  1.02246]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[12.25781, 13.43750],
        [12.92969, 12.97656]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([2826, 8])
pxy:  tensor([[-0.97900, -0.24390],
        [-0.36304, -1.12012]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[11.72656, 12.40625],
        [10.85156, 10.73438]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy.shape:  torch.Size([1834, 8])
pxy:  tensor([[ 0.37183,  0.11499],
        [ 0.41675, -0.04071]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
p_keyxy:  tensor([[10.00781, 10.88281],
        [ 9.17188,  9.95312]], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
before:  tensor([1.92125], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.03288], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.], device='cuda:0') tensor([27.77822], device='cuda:0', grad_fn=<AddBackward0>)
lkey loss:  tensor([0.69446], device='cuda:0', grad_fn=<MulBackward0>)